#+TITLE: prepare
#+DATE: 2014-10-23
#+SETUPFILE: ~/Documents/coding/blog/sven--.github.io/setupfile.org
#+JEKYLL_LAYOUT: post
#+JEKYLL_CATEGORIES: 
#+JEKYLL_PUBLISHED: true
#+OPTIONS: toc:nil

* 기출 키워드
** DS
+ traversal
  + preorder
  + postorder
  + inorder
BST inorder -> incremental order.

binary heap 
BH to implement PQ
array to implement BH
how to find parent / children?
pseudo-code -> insert
sort?

Dijkstra negative?
+ in-place sort?
  + /Merge/
  + /Quick/
  + *Heap*
  + *Selection*
  + *Bubble*
  + *Insertion*
  + /Radix/

+ stable sort?
  + *Merge*
  + /Quick/
  + /Heap/
  + /Selection/
  + *Bubble*
  + *Insertion*
  + *Radix*

Infix to Postfix
Dijkstra O(E log V), data structure?


BST 
+ add
+ array to implement it
+ search, add, delete complexity
+ improvement?



** ARCHI
+ (2)pipeline hazards and resolves
+ N-way set-associative cache
+ design parameter >= 3

+ Average Memory Access factor >= 3
+ multi-level CPU cache

+ "Make the most comman case fast"
+ Instruction Set Architecture
+ Memory System
+ Register Transfer Level

+ (2)Cache Miss Factor = 3
+ treating with cache miss >= 3
+ Average Cache Access Time (hit time, miss rate, miss penalty)

+ direct-mapped cache / 4-way set associative cache
+ Pipeline forwarding / bypassing

+ fully associative cache / 2-way set associative cache

+ Dynamic branch prediction / delayed branch

+ RISC / CISC?
+ exception / procedure call?

+ Pipeline depth?
+ Ahmdal's law, RISC
+ cache, write-through / write-back


** OS
+ algorithms
  + best fit
  + worst fit
  + first fit

DMA effect on system performance >= 2
swap space, existing file system or another partition? if another partition, how to increase performance?

disk read speed

Deadlock, 4 conditions? (\=>? <=? <=>?)
prevent?
avoid?
recovery?


+ Multi-core processor, change?
  + Interrupt handling
  + Scheduling (PCB, queue, etc)
  + Caching Policy
  + File System


scheduling algorithm
SSTF algorithm, Head movement length
Compute bound program, I/O bound program, priority? 
this algorith name?

File IO
+ read, write / memory mapped
buffer cache


THREAD / PROCESS
access control list / access control matrix
SCAN disk scheduling, indefinitely postpone?
I/O bound, compute bound -> which to dispatch first?


* architecture

+ Instruction cycle
http://en.wikipedia.org/wiki/Instruction_cycle
http://en.wikipedia.org/wiki/Instruction_pipeline

+ The classic RISC pipeline comprises:
  + fetch
  + Instruction decode and register fetch
  + Execute
  + Memory access
  + Register write back



[[http://courses.cs.tamu.edu/ejkim/312/Lectures.html][lectures]]
[[http://en.wikibooks.org/wiki/MIPS_Assembly/Instruction_Formats][mips i j k]]
MIPS Assembly/Instruction Formats
+ R instructions are used when all the data values used by the instruction are located in registers.
  + OP rd, rs, rt
  + add $s1, $s2, $s3
  + opcode 	rs 	rt 	rd 	shift (shamt) 	funct
+ I instructions are used when the instruction must operate on an immediate value and a register value. Immediate values may be a maximum of 16 bits long. Larger numbers may not be manipulated by immediate instructions.
  + OP rt, rs, IMM
  + addi $s1, $s2, 100
  + opcode 	rs 	rt 	IMM
+ J instructions are used when a jump needs to be performed. The J instruction has the most space for an immediate value, because addresses are large numbers.
  + OP LABEL
  + Opcode 	Address


+ single cycle processor
#+CAPTION: hihi
#+NAME: single cycle processor
[[http://upload.wikimedia.org/wikipedia/commons/2/2c/Nopipeline.png]]
The length of the cycle must be long enough to accommodate the longest possible propagation delay in the processor. This means that some instructions (typically the arithmetic instructions) will complete quickly, and time will be wasted each cycle. Other instructions (typically memory read or write instructions) will have a much longer propagation delay.

+ multi cycle processor
Multi-cycle processors break up the instruction into its fundamental parts, and executes each part of the instruction in a different clock cycle. Since signals have less distance to travel in a single cycle, the cycle times can be sped up considerably.
  + IF
  Fetch the instruction from memory
  + ID 
  Decode the instruction, and generate the necessary control signals
  + EX 
  Feed the necessary control signals into the ALU and produce a result
  + MEM
  Read from memory, if specified
  + WB 
  Write the result back to the register file or to memory. 





+ Word a computer understands: instruction
+ Vocabulary of all words a computer understands: instruction set (aka instruction set architecture or ISA)
+ Different computers may have different vocabularies (i.e., different ISAs)
  + iPhone (ARM) not same as Macbook (x86)
+ Or the same vocabulary (i.e., same ISA)
  + iPhone and iPad computers have same instruction set (ARM)

+ Why Multiple ISAs?
 + Specialized instructions for specialized applications
 + Different tradeoffs in resources used (e.g., functionality, memory demands, complexity, power consumption, etc.)
 + Competition and innovation is good, especially in emerging environments (e.g., mobile devices)

+ MIPS is a real-world ISA (see www.mips.com)
 + Standard instruction set for networking equipment
 + Was also used in original Nintendo-64!
+ Elegant example of a Reduced Instruction Set Computer (RISC) instruction set

+ Basic RISC principle: "A simpler CPU (the hardware that interprets machine language) is a faster CPU" 
+ Focus of the RISC design is reduction of the number and complexity of instructions in the ISA
+ ARM (Advanced RISC Machine) is most popular RISC

+ Intel 80x86 is another popular ISA and is used in Macbook and PCs (Core i3, Core i5, Core i7, ...)
 + x86 is a Complex Instruction Set Computer (CISC)

+ High-Level Programming languages: could have millions of variables
+ Instruction sets have fixed, small number
+ Called registers
 - "Bricks" of computer hardware
 - Fastest way to store data in computer hardware
 - Visible to (the "assembly language") programmer
+ MIPS Instruction Set has 32 integer registers

+ For registers that hold programmer variables:
  $s0, $s1, $s2, ...
+ For registers that hold temporary variables:
  $t0, $t1, $t2, ...
+ Bit is the atom of Computer Hardware: contains either 0 or 1
+ MIPS registers are 32 bits wide
+ MIPS calls this quantity a word

+ In addition to registers, a computer also has memory that holds millions / billions of words
+ Memory is a single dimension array, starting at 0
+ To access memory, need an address (like an array index)
+ But MIPS arithmetic instructions only operate on registers!
+ Solution: instructions specialized to transfer words (data) between memory and registers
+ Called data transfer instructions


+ Registers: 32 words (128 Bytes)
+ Memory: Billions of bytes (2 GB to 8 GB on laptop)
+ Register access latency : About 100-500 times faster!

+ Conditional Branch
branch if equal (beq) or branch if not equal (bne)
+ Unconditional Branch
a MIPS instruction for this: jump (j)

+ Simplicity favors regularity
 - fixed size instructions
 - small number of instruction formats
 - opcode always the first 6 bits
+ Smaller is faster
 - limited instruction set
 - limited number of registers in register file
 - limited number of addressing modes
+ Make the common case fast
 - arithmetic operands from the register file (load-store machine)
 - allow instructions to contain immediate operands
+ Good design demands good compromises
 - three instruction formats



+ Start fetching and executing the next instruction before the current one has completed
+ Pipelining – modern processors are pipelined for performance
+ CPU time = CPI * CC * IC
 + http://en.wikipedia.org/wiki/Cycles_per_instruction
 + exec time = CPI * Clock Time * Instruction Count
+ Under ideal conditions and with a large number of instructions, the speedup from pipelining is approximately equal to the number of pipe stages
+ A five stage pipeline is nearly five times faster because the CC is nearly five times faster
+ Fetch (and execute) more than one instruction at a time -> Superscala processing


** Pipeline Hazards  
+ structural hazards: attempt to use the same resource by two different instructions at the same time
 + A canonical example is a single memory unit that is accessed both in the fetch stage where an instruction is retrieved from memory, and the memory stage where data is written and/or read from memory. They can often be resolved by separating the component into orthogonal units (such as separate caches) or bubbling the pipeline.
+ data hazards: attempt to use data before it is ready
An instruction's source operand(s) are produced by a prior instruction still in the pipeline
 + RAW, WAW, WAR
+ control hazards: attempt to make a decision about program control flow before the condition has been evaluated and the new PC target address calculated branch and jump instructions, exceptions
 + On many instruction pipeline microarchitectures, the processor will not know the outcome of the branch when it needs to insert a new instruction into the pipeline (normally the fetch stage).

+ Can usually resolve hazards by waiting (=bubble) 
+ pipeline control must detect the hazard and take action to resolve hazards



Data Forwarding (Bypassing)
- Take the result from the earliest point that it exists in any of the pipeline state registers and forward it to the functional units (e.g., the ALU) that need it that cycle
- For ALU functional unit: the inputs can come from any pipeline register rather than just from ID/EX by
- adding multiplexors to the inputs of the ALU
- connecting the Rd write data in EX/MEM or MEM/WB to either (or both)
of the EX’s stage Rs and Rt ALU mux inputs
- adding the proper control hardware to control the new muxes
- Other functional units may need similar forwarding logic (e.g., the DM)
- With forwarding can achieve a CPI of 1 even in the presence of data dependencies


- Along with the Hazard Unit, we have to implement the stall
- Prevent the instructions in the IF and ID stages from progressing down the pipeline – done by preventing the PC register and the IF/ID pipeline register from changing
- Hazard detection Unit controls the writing of the PC (PC.write) and IF/I D (IF/ID.write) registers
- Insert a “bubble” between the lw instruction (in the EX stage) and the load-use instruction (in the ID stage) (i.e., insert a noop in the execution stream)
- Set the control bits in the EX, MEM, and WB control fields of the ID/EX pipeline register to 0 (noop). The Hazard Unit controls the mux that choose s between the real control values and the 0’s.
- Let the lw instruction and the instructions after it in the pipeline (before it in the code) proceed normally down the pipeline








** eliminating
http://en.wikipedia.org/wiki/Hazard_%28computer_architecture%29#Eliminating_hazards
Pipeline bubbling
Main article: Bubble (computing)

Bubbling the pipeline, also known as a pipeline break or a pipeline stall, is a method for preventing data, structural, and branch hazards from occurring. As instructions are fetched, control logic determines whether a hazard could/will occur. If this is true, then the control logic inserts NOPs into the pipeline. Thus, before the next instruction (which would cause the hazard) is executed, the previous one will have had sufficient time to complete and prevent the hazard. If the number of NOPs is equal to the number of stages in the pipeline, the processor has been cleared of all instructions and can proceed free from hazards. All forms of stalling introduce a delay before the processor can resume execution.

Flushing the pipeline occurs when a branch instruction jumps to a new memory location, invalidating all previous stages in the pipeline. These previous stages are cleared allowing the pipeline to continue at the new instruction indicated by the branch.
Data hazards

There are several main solutions and algorithms used to resolve data hazards:

    insert a pipeline bubble whenever a read after write (RAW) dependency is encountered, guaranteed to increase latency, or
    utilize out-of-order execution to potentially prevent the need for pipeline bubbles
    utilize operand forwarding to use data from later stages in the pipeline

In the case of out-of-order execution, the algorithm used can be:

    scoreboarding, in which case a pipeline bubble will only be needed when there is no functional unit available
    the Tomasulo algorithm, which utilizes register renaming allowing the continual issuing of instructions

We can delegate the task of removing data dependencies to the compiler, which can fill in an appropriate number of NOP instructions between dependent instructions to ensure correct operation, or re-order instructions where possible.
Operand forwarding
Main article: Operand forwarding

Forwarding involves feeding output data into a previous stage of the pipeline. Forwarding is implemented by feeding back the output of an instruction into the previous stage(s) of the pipeline as soon as the output of that instruction is available.
Examples

    NOTE: In the following examples, computed values are in bold, while Register numbers are not.

For instance, let's say we want to write the value 3 to register 1, (which already contains a 6), and then add 7 to register 1 and store the result in register 2, i.e.:

    Instruction 0: Register 1 = 6
    Instruction 1: Register 1 = 3
    Instruction 2: Register 2 = Register 1 + 7 = 10

Following execution, register 2 should contain the value 10. However, if Instruction 1 (write 3 to register 1) does not completely exit the pipeline before Instruction 2 starts execution, it means that Register 1 does not contain the value 3 when Instruction 2 performs its addition. In such an event, Instruction 2 adds 7 to the old value of register 1 (6), and so register 2 would contain 13 instead, i.e.:

    Instruction 0: Register 1 = 6
    Instruction 2: Register 2 = Register 1 + 7 = 13
    Instruction 1: Register 1 = 3

This error occurs because Instruction 2 reads Register 1 before Instruction 1 has committed/stored the result of its write operation to Register 1. So when Instruction 2 is reading the contents of Register 1, register 1 still contains 6, not 3.

Forwarding (described below) helps correct such errors by depending on the fact that the output of Instruction 1 (which is 3) can be used by subsequent instructions before the value 3 is committed to/stored in Register 1.

Forwarding applied to our example means that we do not wait to commit/store the output of Instruction 1 in Register 1 (in this example, the output is 3) before making that output available to the subsequent instruction (in this case, Instruction 2). The effect is that Instruction 2 uses the correct (the more recent) value of Register 1: the commit/store was made immediately and not pipelined.

With forwarding enabled, the ID/EX or Instruction Decode/Execution stage of the pipeline now has two inputs: the value read from the register specified (in this example, the value 6 from Register 1), and the new value of Register 1 (in this example, this value is 3) which is sent from the next stage (EX/MEM) or Instruction Execute/Memory Access. Additional control logic is used to determine which input to use.
Control hazards (branch hazards)

To avoid control hazards microarchitectures can:

    insert a pipeline bubble (discussed above), guaranteed to increase latency, or
    use branch prediction and essentially make educated guesses about which instructions to insert, in which case a pipeline bubble will only be needed in the case of an incorrect prediction

In the event that a branch causes a pipeline bubble after incorrect instructions have entered the pipeline, care must be taken to prevent any of the wrongly-loaded instructions from having any effect on the processor state excluding energy wasted processing them before they were discovered to be loaded incorrectly.

http://en.wikipedia.org/wiki/Hazard_%28computer_architecture%29
http://www.cs.iastate.edu/~prabhu/Tutorial/PIPELINE/hazards.html







** cache
Fact: Large memories are slow and fast memories are small
How do we create a memory that gives the illusion of being large, cheap and fast (most of the time)?
+ With hierarchy
+ With parallelism

+ Caches use SRAM for speed and technology compatibility
 + Fast (typical access times of 0.5 to 2.5 nsec)
 + Low density (6 transistor cells), higher power, expensive ($2000 to $5000 per GB in 2008)
 + Static: content will last "forever" (as long as power is left on)
+ Main memory uses DRAM for size (density)
 + Slower (typical access times of 50 to 70 nsec)
 + High density (1 transistor cells), lower power, cheaper ($20 to $75 per GB in 2008)
 + Dynamic: needs to be "refreshed" regularly (~ every 8 ms)
- consumes 1% to 2% of the active cycles of the DRAM


+ Spatial locality, Temporal locality


- Block (or line): the minimum unit of information that is present (or not) in a cache
- Hit Rate: the fraction of memory accesses found in a level of the memory hierarchy
- Hit Time: Time to access that level which consists of 
   Time to access the block + 
   Time to determine hit/miss
- Miss Rate: the fraction of memory accesses not found in a level of the memory hierarchy => 1 - (Hit Rate)
- Miss Penalty: Time to replace a block in that level with the corresponding block from a lower level which consists of
 - Time to access the block in the lower level + 
   Time to transmit that block to the level that experienced the miss + 
   Time to insert the block in that level + 
   Time to pass the block to the requestor
- Hit Time << Miss Penalty

** How is the Hierarchy Managed?
1. registers <-> memory
 - by compiler (programmer?)
2. cache <-> main memory
 - by the cache controller hardware
3. main memory <-> disks
 - by the operating system (virtual memory)
 - virtual to physical address mapping assisted by the hardware (TLB)
 - by the programmer (files)



1. Processor registers - the fastest possible access (usually 1 CPU cycle). A few thousand bytes in size
2. Cache
3. Main memory
4. Disk storage




** 
- Write-Through
 - On cache write - always update main memory as well
 - Use a write buffer to stockpile writes to main memory for speed
- Write-Back
 - On cache write - remember that block is modified (dirty bit)
 - Update main memory when dirty block is replaced
 - Sometimes need to flush cache (I/O, multiprocessing)


+ Average Memory Access factor >= 3
+ Average Cache Access Time (hit time, miss rate, miss penalty)
 
+ Hit Time
 + Smaller cache
 + Smaller blocks
 + Use direct-mapped cache.
 + Write buffers speed up write-through caches.
 + Write - pipeline
+ Miss Penalty
 + Smaller block
 + Use additional levels of cache
 + multi-level
 + dirty bit
 + critical word first
+ Miss Rate
 + Bigger cache
 + Set Associative Cache
 + multi-level CPU cache



+ (2)Cache Miss Factor = 3
+ treating with cache miss >= 3

+ Compulsory (cold start or process migration, first reference):
 - First access to a block, "cold" fact of life, not a whole lot you can do about it. If you are going to run "millions" of instruction, compulsory misses are insignificant
 - Solution: increase block size (increases miss penalty; very large blocks could increase miss rate)
+ Capacity:
 - Cache cannot contain all blocks accessed by the program
 - Solution: increase cache size (may increase access time)
+ Conflict (collision):
 + Multiple memory locations mapped to the same cache location
 - Solution 1: increase cache size
 - Solution 2: increase associativity (stay tuned) (may increase access time)



* OS
Both processes and threads are independent sequences of execution. 
+ threads (of the same process) run in a shared memory space
+ processes run in separate memory spaces.

Threads differ from traditional multitasking operating system processes in that:
+ processes are typically independent, while threads exist as subsets of a process
+ processes carry considerably more state information than threads, whereas multiple threads within a process share process state as well as memory and other resources
+ processes have separate address spaces, whereas threads share their address space
+ processes interact only through system-provided inter-process communication mechanisms
+ context switching between threads in the same process is typically faster than context switching between processes.

Kernel-level threading
User-level threading
Hybrid threading




+ Memory Alloctaion algorithms
  + best fit
  + worst fit
  + first fit



+ Necessary Deadlock
 + Mutual Exclusion : At least one resource must be held in a non-shareable mode.


*PREVENTION*  
 + Hold and Wait | Resource Holding : A process is currently holding at least one resource and requesting additional resources which are being held by other processes.
  + requiring processes to request all the resources they will need before starting up
 + No Preemtion : a resource can be released only voluntarily by the process holding it, after that process has completed its task
 + Circular wait : 
  + One way to avoid circular wait is to number all resources, and to require that processes request resources only in strictly increasing ( or decreasing ) order. 

*AVOIDANCE*
Safe State
Resource-Allocation Graph Algorithm
Banker's Algorithm


*RECOVERY*
Process Termination
Resource Preemption 


+ Deadlock prevention or avoidance - Do not allow the system to get into a deadlocked state.
+ Deadlock detection and recovery - Abort a process or preempt some resources when deadlocks are detected.
prevent?
avoid?
recovery?


DMA effect on system performance >= 2
swap space, existing file system or another partition? if another partition, how to increase performance?

disk read speed

Deadlock, 4 conditions? (\=>? <=? <=>?)
prevent?
avoid?
recovery?


+ Multi-core processor, change?
  + Interrupt handling
  + Scheduling (PCB, queue, etc)
  + Caching Policy
  + File System


scheduling algorithm
SSTF algorithm, Head movement length
Compute bound program, I/O bound program, priority? 
this algorith name?

File IO
+ read, write / memory mapped
buffer cache


THREAD / PROCESS
access control list / access control matrix
SCAN disk scheduling, indefinitely postpone?
I/O bound, compute bound -> which to dispatch first?



{{{more}}}


















